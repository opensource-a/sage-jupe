Parameters:
  PermissionsBoundary:
    Type: String
    Default: 'arn:aws:iam::aws:policy/AdministratorAccess'
    Description: Permissions Boundary ARN for all roles 
  S3BucketName:
    Type: String
    Description: Bucket for input, output, notebook files     
  S3InputPrefix:
    Type: String
    Default: 'input'
    Description: Bucket prefix for input files
  S3OutputPrefix:
    Type: String
    Default: 'output'
    Description: Bucket prefix for output files
  S3NotebookPrefix:
    Type: String
    Default: 'notebook'
    Description: Bucket prefix for notebook files
  S3NotebookKey:
    Type: String
    Default: 'notebook.ipynb'
    Description: Notebook file key 
  ProcessingInstanceType:
    Type: String
    Default: 'ml.m5.large'
    Description: Processing Instance Type
  ProcessingJobSecurityGroup:
    Type: String
    Description: Processing Job Security Group
  ProcessingJobSubnetId:
    Type: String
    Description: Processing Job Subnet Id    

Resources:
  AthenaWorkGroup:
    Type: AWS::Athena::WorkGroup
    Properties:
      Name: !Sub '${AWS::StackName}-athena-workgroup-${AWS::AccountId}'
      State: ENABLED
      RecursiveDeleteOption: true
      WorkGroupConfiguration:
        EnforceWorkGroupConfiguration: false
        PublishCloudWatchMetricsEnabled: true
        RequesterPaysEnabled: true
        ResultConfiguration:
          OutputLocation: !Sub 's3://${S3BucketName}/output'
  GlueSchema:
    Type: AWS::Glue::Schema
    Properties: 
      Compatibility: DISABLED
      DataFormat: AVRO
      Name: !Sub '${AWS::StackName}-glue-schema-${AWS::AccountId}'
      SchemaDefinition: '{"type" : "record","name" : "Cities", "fields" : [{"type": "string","name": "city"}, {"type": "string","name": "city_ascii"}, {"type": "double","name": "lat"}, {"type": "double", "name": "lng"}, {"type": "string","name": "country"},{"type": "string", "name": "iso2"}, {"type": "string", "name": "iso3"}, {"type": "string", "name": "admin_name"}, {"type": "string","name": "capital"},{"type": "double", "name": "population"},{"type": "double", "name": "id"}]}'
  GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: !Sub '${AWS::StackName}-glue-db-${AWS::AccountId}'
  GlueTable:
    # Creating the table waits for the database to be created
    DependsOn: 
    - GlueDatabase
    - S3Bucket
    Type: AWS::Glue::Table
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseName: !Ref GlueDatabase
      TableInput:
        Name: !Sub '${AWS::StackName}-glue-table-${AWS::AccountId}'
        TableType: EXTERNAL_TABLE
        Parameters: {"classification": "csv", "delimiter": ",", "skip.header.line.count": "1"}
        StorageDescriptor:
          SchemaReference:
            SchemaVersionNumber: 1
            SchemaId: 
              SchemaArn: !GetAtt GlueSchema.Arn
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          Location: !Sub 's3://${S3BucketName}/input'
          SerdeInfo:
            Parameters:
              field.delim: ","
            SerializationLibrary: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  S3Bucket:
    Type: 'AWS::S3::Bucket'
    DeletionPolicy: Retain
    DependsOn: 
    - BucketPermission
    Properties:
      BucketName: !Ref S3BucketName
      NotificationConfiguration:
        LambdaConfigurations:
        - Event: 's3:ObjectCreated:*'
          Function: !GetAtt InvokeNotebookLambda.Arn      
          Filter:
              S3Key:
                Rules:
                  - Name: prefix
                    Value: 'input'          


  ExecuteNotebookClientRole:
    Type: 'AWS::IAM::Role'
    Properties:
      RoleName: !Sub '${AWS::StackName}-ExecuteNotebookClientRole'
      PermissionsBoundary: !Ref PermissionsBoundary
      Description: >-
        A minimal role that lets the user run notebooks on demand or on a
        scheduler
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - sagemaker.amazonaws.com
            Action:
              - 'sts:AssumeRole'
  ExecuteNotebookClientPolicy:
    Type: 'AWS::IAM::Policy'
    Properties:
      PolicyName: !Join 
        - '-'
        - - ExecuteNotebookClient
          - !Ref 'AWS::Region'
      Roles:
        - !Ref ExecuteNotebookClientRole
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action: '*'
            Resource: '*'
 
  BasicExecuteNotebookRole:
    Type: 'AWS::IAM::Role'
    Properties:
      PermissionsBoundary: !Ref PermissionsBoundary    
      RoleName: !Sub '${AWS::StackName}-BasicExecuteNotebookRole'
      Description: >-
        A minimal role used as the default for running the notebook container in
        SageMaker Processing
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - sagemaker.amazonaws.com
            Action:
              - 'sts:AssumeRole'
  ExecuteNotebookContainerPolicy:
    Type: 'AWS::IAM::Policy'
    Properties:
      PolicyName: !Join 
        - '-'
        - - ExecuteNotebookContainerPolicy
          - !Ref 'AWS::Region'
      Roles:
        - !Ref BasicExecuteNotebookRole
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action: '*'
            Resource: '*'

  ContainerBuildRole:
    Type: 'AWS::IAM::Role'
    Properties:
      PermissionsBoundary: !Ref PermissionsBoundary    
      RoleName: !Sub '${AWS::StackName}-ContainerBuildRole'
      Description: The role for building containers to be used with sagemaker_run_notebook
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - codebuild.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action: '*'
                Resource: '*'

  LambdaExecutionRole:
    Type: 'AWS::IAM::Role'
    Properties:
      Description: The role for running the sagemaker_run_notebook lambda
      RoleName: !Sub '${AWS::StackName}-RunNotebook-LambdaExecutionRole'
      PermissionsBoundary: !Ref PermissionsBoundary      
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Policies:
        - PolicyName: root
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action: '*'
                Resource: '*'
  BucketPermission:
    Type: AWS::Lambda::Permission
    DependsOn: 
    - InvokeNotebookLambda   
    Properties:
      Action: 'lambda:InvokeFunction'
      FunctionName: !Ref InvokeNotebookLambda
      Principal: s3.amazonaws.com
      SourceAccount: !Ref "AWS::AccountId"
      SourceArn: !Sub "arn:aws:s3:::${S3BucketName}"
  InvokeNotebookLambda:
    Type: 'AWS::Lambda::Function'
    DependsOn: 
    - LambdaExecutionRole
    - ExecuteNotebookClientRole
    - BasicExecuteNotebookRole
    - ContainerBuildRole
    Properties:
      FunctionName: !Sub '${AWS::StackName}-RunNotebook'
      Description: A function to run Jupyter notebooks using SageMaker processing jobs
      Handler: index.lambda_handler
      Runtime: python3.7
      Role: !GetAtt 
        - LambdaExecutionRole
        - Arn
      Timeout: 30
      Code:
        ZipFile: !Sub |
          import os
          import json
          import boto3
          import time

          from typing import Optional


          sm = boto3.client('sagemaker')


          def get_unique_job_name(base_name: str):
              """ Returns a unique job name based on a given base_name
                  and the current timestamp """
              timestamp = time.strftime('%Y%m%d-%H%M%S')
              return f'{base_name}-{timestamp}'


          def get_file_input(name: str, input_s3_uri: str, output_path: str):
              """ Returns the input file configuration
                  Modify if you need different input method """
              return {
                  'InputName': name,
                  'S3Input': {
                      'S3Uri': input_s3_uri,
                      'LocalPath': output_path,
                      'S3DataType': 'S3Prefix',
                      'S3InputMode': 'File'
                  }
              }

          def get_file_output(name: str, local_path: str, ouput_s3_uri: str):
              """ Returns output file configuration
                  Modify for different output method """
              return {
                  'OutputName': name,
                  'S3Output': {
                      'S3Uri': ouput_s3_uri,
                      'LocalPath': local_path,
                      'S3UploadMode': 'EndOfJob'
                  }
              }


          def get_app_spec(image_uri: str, container_arguments: Optional[str], entrypoint: Optional[str]):
              app_spec = {
                  'ImageUri': image_uri
              }

              if container_arguments is not None:
                  app_spec['ContainerArguments'] = container_arguments

              if entrypoint is not None:
                  # Similar to ScriptProcessor in sagemaker SDK:
                  # Run a custome script within the container
                  app_spec['ContainerEntrypoint'] = ['python', entrypoint]

              return app_spec


          def lambda_handler(event, context):

              # (1) Get inputs
              K=ensure_session();L=K.region_name;M=K.client('sts').get_caller_identity()['Account']
              
              #input_uri = event.get('S3Input', "s3://"
              #ouput_uri = event['S3Output']
              image_uri = event.get('ImageUri', f"{M}.dkr.ecr.{L}.amazonaws.com/{A}:latest")
              print(image_uri)
              script_uri = event.get('S3Script', "s3://${S3BucketName}/${S3NotebookPrefix}/${S3NotebookKey}")  # Optional: S3 path to custom script
              print(script_uri)
              # Get execution environment
              role = event.get('RoleArn', "${BasicExecuteNotebookRole.Arn}")
              print(role)
              instance_type = event.get('InstanceType', "${ProcessingInstanceType}")
              print(instance_type)
              volume_size = event.get('VolumeSizeInGB', 100)
              max_runtime = event.get('MaxRuntimeInSeconds', 3600)  # Default: 1h
              container_arguments = event.get('ContainerArguments', None) # Optional: Arguments to pass to the container
              security_group = event.get('SecurityGroup', "${ProcessingJobSecurityGroup}")
              print(security_group)
              subnet_id = event.get('SubnetId', "${ProcessingJobSubnetId}")
              print(subnet_id)
              entrypoint = None  # Entrypoint to the container, will be set automatically later

              job_name = get_unique_job_name('sm-processing-job')  # (2)
              

              #
              # (3) Specify inputs / Outputs
              #

              #inputs = [
              #    get_file_input('data', input_uri, '/opt/ml/processing/input')
              #]
              inputs = []

              if script_uri is not None:
                  # Add custome script to the container (similar to ScriptProcessor)
                  inputs.append(get_file_input('script', script_uri, '/opt/ml/processing/code'))

                  # Make script new entrypoint for the container
                  filename = os.path.basename(script_uri)
                  entrypoint = f'/opt/ml/processing/code/{filename}'

              #outputs = [
              #    get_file_output('output_data', '/opt/ml/processing/output', ouput_uri)
              #]

              #
              # Define execution environment
              #

              app_spec = get_app_spec(image_uri, container_arguments, entrypoint)

              cluster_config = {
                  'InstanceCount': 1,
                  'InstanceType': instance_type,
                  'VolumeSizeInGB': volume_size
              }

              #
              # (4) Create processing job and return job ARN
              #
              sm.create_processing_job(
                  ProcessingInputs=inputs,
                  #ProcessingOutputConfig={
                  #    'Outputs': outputs
                  #},
                  ProcessingJobName=job_name,
                  ProcessingResources={
                      'ClusterConfig': cluster_config
                  },
                  StoppingCondition={
                      'MaxRuntimeInSeconds': max_runtime
                  },
                  NetworkConfig={
                      'VpcConfig': {
                          'SecurityGroupIds': [
                              security_group,
                           ],
                           'Subnets': [
                               subnet_id,
                           ]
                      }
                  },
                  AppSpecification=app_spec,
                  RoleArn=role
              )

              return {
                  'ProcessingJobName': job_name
              }
